{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theovercomer8/BLIP/blob/main/TO8_Captionr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Captionr by theovercomer8\n",
        "You can find the latest update to the notebook [here](https://github.com/theovercomer8/captionr).\n"
      ],
      "metadata": {
        "id": "OGgQmF2rO1DY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ie6oPstv4Xx5"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "#@markdown Installs requirements and imports CLIP data\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/data'):\n",
        "  !rm -rf /content/data\n",
        "\n",
        "\n",
        "if os.path.exists('/content/requirements.txt'):\n",
        "  !rm /content/requirements.txt\n",
        "\n",
        "!mkdir /content/data\n",
        "%cd /content/data\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/dataset'):\n",
        "  !mkdir /content/dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/captionr/main/data/artists.txt\n",
        "!wget https://raw.githubusercontent.com/jvkap/clip-interrogator/main/clip_interrogator/data/flavors.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/captionr/main/data/movements.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/captionr/main/data/mediums.txt\n",
        "\n",
        "%cd /content\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/captionr/main/data/requirements.txt\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!apt-get install aria2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P8Zrmzbu5gEP"
      },
      "outputs": [],
      "source": [
        "#@title Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Do6uhfD85t91"
      },
      "outputs": [],
      "source": [
        "#@title Download and Extract Zip (.zip)\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "root_dir = '/content'\n",
        "#@markdown ### Define Zipfile URL or Zipfile Path\n",
        "zipfile_url_or_path = \"\" #@param {'type': 'string'}\n",
        "zipfile_dst = str(root_dir)+\"/zip_file.zip\"\n",
        "extract_to = \"/content/dataset\" #@param {'type': 'string'}\n",
        "\n",
        "if extract_to != \"\":\n",
        "  os.makedirs(extract_to, exist_ok=True)\n",
        "else:\n",
        "  extract_to = \"/content/dataset\"\n",
        "\n",
        "#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`\n",
        "is_dataset = False #@param{'type':'boolean'}\n",
        "\n",
        "#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory\n",
        "auto_unzip_and_delete = False #@param{'type':'boolean'}\n",
        "\n",
        "dirname = os.path.dirname(zipfile_dst)\n",
        "basename = os.path.basename(zipfile_dst)\n",
        "\n",
        "try:\n",
        "  if zipfile_url_or_path.startswith(\"/content\"):\n",
        "    zipfile_dst = zipfile_url_or_path\n",
        "    if auto_unzip_and_delete == False:\n",
        "      if is_dataset:\n",
        "        extract_to = \"/content/dataset\"\n",
        "      !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "  elif zipfile_url_or_path.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"magnet:?\"):\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in zipfile_url_or_path:\n",
        "      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = write_token\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"An error occurred while downloading the file:\", e)\n",
        "\n",
        "if is_dataset:\n",
        "  extract_to = '/content/dataset'\n",
        "\n",
        "if auto_unzip_and_delete:\n",
        "  !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "\n",
        "  path_obj = Path(zipfile_dst)\n",
        "  zipfile_name = path_obj.parts[-1]\n",
        "  \n",
        "  if os.path.isdir(zipfile_dst):\n",
        "    print(\"\\nThis zipfile doesn't exist or has been deleted \\n\")\n",
        "  else:\n",
        "    os.remove(zipfile_dst)\n",
        "    print(f\"\\n{zipfile_name} has been deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption Wizard\n",
        "\n",
        "from json import JSONDecoder, JSONEncoder\n",
        "import sys\n",
        "import getopt\n",
        "import time\n",
        "import os, subprocess\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM, CLIPFeatureExtractor, CLIPModel\n",
        "import hashlib\n",
        "import inspect\n",
        "import math\n",
        "import numpy as np\n",
        "import open_clip\n",
        "import pickle\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from blip.models.blip import blip_decoder, BLIP_Decoder\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from   IPython.core.display import display, HTML\n",
        "import gc\n",
        "\n",
        "\n",
        "# All objects we find\n",
        "json_found = []  \n",
        "# raw_decode expects byte1 to be part of a JSON, so remove whitespace from left\n",
        "# stdin = sys.stdin.read().lstrip()\n",
        "decoder = JSONDecoder()\n",
        "encoder = JSONEncoder()\n",
        "\n",
        "#@markdown Folder to scan for images\n",
        "folder_path = '/content/dataset' #@param {type:\"string\"}\n",
        "#@markdown Folder to output captions. Captioned will be named the same as the input image with a .txt extension\n",
        "output_path = '/content/dataset' #@param {type:\"string\"}\n",
        "#@markdown Action to take for existing caption files\n",
        "existing = 'skip' #@param [ 'skip', 'ignore', 'copy', 'prepend', 'append']\n",
        "#@markdown Max caption length\n",
        "cap_length = 150 #@param {type: \"slider\", min: 0, max: 400}\n",
        "#@markdown ---\n",
        "git_pass = True #@param {type:\"boolean\"}\n",
        "#@markdown If GIT Pass enabled, fail the caption and move on to BLIP/Coca pass if caption contains any of the following phrases (comma delimted)\n",
        "git_fail_phrases = 'a sign that says,writing that says,that says,with the word' #@param {type:\"string\"}\n",
        "#@markdown Perform a BLIP or Coca caption pass\n",
        "second_pass = True #@param {type:\"boolean\"}\n",
        "second_pass_type = 'Coca' #@param ['BLIP', 'Coca']\n",
        "#@markdown ---\n",
        "#@markdown BLIP options if BLIP selected, ignored if Coca selected\n",
        "blip_beams = 64 #@param {type: \"slider\", min: 1, max: 100}\n",
        "blip_min = 30 #@param {type: \"slider\", min: 5, max: 75}\n",
        "blip_max = 56 #@param {type: \"slider\", min: 5, max: 75}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "clip_model_name = 'ViT-H-14/laion2b_s32b_b79k' #@param ['ViT-L-14/openai',\"ViT-H-14/laion2b_s32b_b79k\"]\n",
        "\n",
        "#@markdown Use ViT-H for SD 2.x, ViT-L for SD 1.5\n",
        "#@markdown\n",
        "#@markdown Only used if one of the following flavor/artist/medium/movement/trending checkboxes are checked\n",
        "\n",
        "clip_use_flavor = True #@param {type:\"boolean\"}\n",
        "clip_max_flavors = 8 #@param {type: \"slider\", min: 1, max: 100}\n",
        "clip_use_artist = False #@param {type:\"boolean\"}\n",
        "clip_use_medium = False #@param {type:\"boolean\"}\n",
        "clip_use_movement = False #@param {type:\"boolean\"}\n",
        "clip_use_trending = False  #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown Tags to ignore if they are returned by CLIP\n",
        "ignore_tags = '' #@param {type:\"string\"}\n",
        "#@markdown Find/replace in caption. Set `replace_class` to true to use. Will replace any instances of `sub_class` with `sub_name`\n",
        "replace_class = False #@param {type:\"boolean\"}\n",
        "sub_class = '' #@param {type:\"string\"}\n",
        "sub_name = '' #@param {type:\"string\"}\n",
        "#@markdown Tag the caption with the containing folder. Useful when using nested folder structure. Can tag up to `folder_tag_levels` deep\n",
        "folder_tag = False #@param {type:\"boolean\"}\n",
        "folder_tag_levels = 1 #@param {type: \"slider\", min: 1, max: 10}\n",
        "uniquify_tags = True #@param {type:\"boolean\"}\n",
        "#@markdown Tags to prepend or append to the generated caption. Useful for adding a subject or style\n",
        "prepend_tags = '' #@param {type:\"string\"}\n",
        "append_tags = '' #@param {type:\"string\"}\n",
        "#@markdown Uncheck `write_to_file` to just perform a preview run\n",
        "write_to_file = True #@param {type:\"boolean\"}\n",
        "#@markdown Read caption from filename if caption file does not exist\n",
        "use_filename = False #@param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
        "try:\n",
        "  clip_loaded\n",
        "except NameError:\n",
        "  clip_loaded = False\n",
        "  _clip_model = None\n",
        "  _clip_preprocess = None\n",
        "try:\n",
        "  blip_loaded\n",
        "except NameError:\n",
        "  blip_loaded = False  \n",
        "  _blip_model = None\n",
        "  \n",
        "try:\n",
        "  coca_loaded\n",
        "except NameError:\n",
        "  coca_loaded = False  \n",
        "  coca_processor = None\n",
        "  coca_model = None\n",
        "try:\n",
        "  git_loaded\n",
        "except NameError:\n",
        "  git_loaded = False  \n",
        "  processor = None\n",
        "  model = None\n",
        "\n",
        "\n",
        "\n",
        "git_model_name = \"microsoft/git-large-r-textcaps\"\n",
        "\n",
        "try:\n",
        "    cached_for\n",
        "except NameError:\n",
        "    cached_for = []\n",
        "    \n",
        "if cached_for is None or clip_model_name not in cached_for:\n",
        "  print(\"Download preprocessed cache files...\")\n",
        "  CACHE_URLS = [\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-L-14_openai_artists.pkl',\n",
        "      'https://github.com/theovercomer8/captionr/raw/main/data/ViT-L-14_openai_flavors.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-L-14_openai_mediums.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-L-14_openai_movements.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-L-14_openai_trendings.pkl',\n",
        "  ] if clip_model_name == 'ViT-L-14/openai' else [\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_artists.pkl',\n",
        "      'https://github.com/theovercomer8/captionr/raw/main/data/ViT-H-14_laion2b_s32b_b79k_flavors.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_mediums.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_movements.pkl',\n",
        "      'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_trendings.pkl',\n",
        "  ]\n",
        "  os.makedirs('cache', exist_ok=True)\n",
        "  for url in CACHE_URLS:\n",
        "      subprocess.run(['wget', url, '-P', 'cache'], stdout=subprocess.PIPE)\n",
        "  cached_for.append(clip_model_name)\n",
        "\n",
        "def get_parent_folder(filepath, levels=1):\n",
        "    common = os.path.split(filepath)[0]\n",
        "    paths = []\n",
        "    for i in range(int(levels)):\n",
        "        split = os.path.split(common)\n",
        "        common = split[0]\n",
        "        paths.append(split[1])\n",
        "    return paths\n",
        "\n",
        "def coca_caption(img):\n",
        "    im = coca_processor(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "      generated = coca_model.generate(im)\n",
        "\n",
        "    generated_caption = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
        "    return generated_caption\n",
        "\n",
        "def git_caption(img):\n",
        "    pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    generated_ids = model.generate(pixel_values=pixel_values, max_length=cap_length)\n",
        "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "BLIP_MODELS = {\n",
        "    'base': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth',\n",
        "    'large': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "}\n",
        "\n",
        "@dataclass \n",
        "class Config:\n",
        "    # models can optionally be passed in directly\n",
        "    blip_model: BLIP_Decoder = None\n",
        "    clip_model = None\n",
        "    clip_preprocess = None\n",
        "\n",
        "    # blip settings\n",
        "    blip_image_eval_size: int = 384\n",
        "    blip_model_type: str = 'large' # choose between 'base' or 'large'\n",
        "    blip_offload: bool = False\n",
        "\n",
        "    # clip settings\n",
        "    clip_model_path: str = None\n",
        "\n",
        "    # interrogator settings\n",
        "    cache_path: str = 'cache'\n",
        "    chunk_size: int = 2048 if clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    data_path: str = '/content/data'\n",
        "    device: str = (\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    flavor_intermediate_count: int = 2048 if clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    quiet: bool = True # when quiet progress bars are not shown\n",
        "\n",
        "class Interrogator():\n",
        "    def __init__(self, config: Config):\n",
        "        global blip_loaded,_blip_model,clip_loaded,_clip_model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        if second_pass and second_pass_type == 'BLIP':\n",
        "            if config.blip_model is None:\n",
        "                print(\"Loading BLIP model...\")\n",
        "                blip_path = os.path.dirname(inspect.getfile(blip_decoder))\n",
        "                configs_path = os.path.join(os.path.dirname(blip_path), 'configs')\n",
        "                med_config = os.path.join(configs_path, 'med_config.json')\n",
        "                blip_model = blip_decoder(\n",
        "                    pretrained=BLIP_MODELS[config.blip_model_type],\n",
        "                    image_size=config.blip_image_eval_size, \n",
        "                    vit=config.blip_model_type, \n",
        "                    med_config=med_config\n",
        "                )\n",
        "                blip_model.eval()\n",
        "                blip_model = blip_model.to(config.device)\n",
        "                self.blip_model = blip_model\n",
        "                blip_loaded = True\n",
        "                _blip_model = blip_model\n",
        "            else:\n",
        "                self.blip_model = config.blip_model\n",
        "\n",
        "        if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending:\n",
        "            self.load_clip_model()\n",
        "\n",
        "    def load_clip_model(self):\n",
        "        global clip_loaded, _clip_model, _clip_preprocess\n",
        "        start_time = time.time()\n",
        "        config = self.config\n",
        "\n",
        "        cmn, clip_model_pretrained_name = clip_model_name.split('/', 2)\n",
        "        if config.clip_model is None:\n",
        "            print(f\"Loading CLIP model...\")\n",
        "\n",
        "            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
        "                cmn, \n",
        "                pretrained=clip_model_pretrained_name, \n",
        "                precision='fp16' if config.device == 'cuda' else 'fp32',\n",
        "                device=config.device,\n",
        "                jit=False,\n",
        "                cache_dir=config.clip_model_path\n",
        "            )\n",
        "            self.clip_model.to(config.device).eval()\n",
        "            clip_model = self.clip_model\n",
        "            clip_loaded = True\n",
        "            _clip_model = clip_model\n",
        "            _clip_preprocess = self.clip_preprocess\n",
        "        else:\n",
        "            self.clip_model = config.clip_model\n",
        "            self.clip_preprocess = config.clip_preprocess\n",
        "        self.tokenize = open_clip.get_tokenizer(cmn)\n",
        "\n",
        "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "        trending_list = [site for site in sites]\n",
        "        trending_list.extend([\"trending on \"+site for site in sites])\n",
        "        trending_list.extend([\"featured on \"+site for site in sites])\n",
        "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "        raw_artists = _load_list(config.data_path, 'artists.txt')\n",
        "        artists = [f\"by {a}\" for a in raw_artists]\n",
        "        artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
        "\n",
        "        if clip_use_artist:\n",
        "            self.artists = LabelTable(artists, \"artists\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_flavor:\n",
        "            self.flavors = LabelTable(_load_list(config.data_path, 'flavors.txt'), \"flavors\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_medium:\n",
        "            self.mediums = LabelTable(_load_list(config.data_path, 'mediums.txt'), \"mediums\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_movement:\n",
        "            self.movements = LabelTable(_load_list(config.data_path, 'movements.txt'), \"movements\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_trending:\n",
        "            self.trendings = LabelTable(trending_list, \"trendings\", self.clip_model, self.tokenize, config)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Loaded CLIP model and data in {end_time-start_time:.2f} seconds.\")\n",
        "\n",
        "    def generate_blip_caption(self, pil_image: Image) -> str:\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(self.device)\n",
        "        size = self.config.blip_image_eval_size\n",
        "        gpu_image = transforms.Compose([\n",
        "            transforms.Resize((size, size), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            caption = self.blip_model.generate(\n",
        "                gpu_image, \n",
        "                sample=False, \n",
        "                num_beams=clip_beams, \n",
        "                max_length=clip_max, \n",
        "                min_length=clip_min\n",
        "            )\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(\"cpu\")\n",
        "        return caption[0]\n",
        "\n",
        "    def image_to_features(self, image: Image) -> torch.Tensor:\n",
        "        images = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = self.clip_model.encode_image(images)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features\n",
        "\n",
        "    \n",
        "    def interrogate(self, caption: str, image: Image) -> str:\n",
        "        image_features = self.image_to_features(image)\n",
        "\n",
        "        # flaves = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "        # best_medium = self.mediums.rank(image_features, 1)[0]\n",
        "        # best_artist = self.artists.rank(image_features, 1)[0]\n",
        "        # best_trending = self.trendings.rank(image_features, 1)[0]\n",
        "        # best_movement = self.movements.rank(image_features, 1)[0]\n",
        "\n",
        "        best_prompt = caption\n",
        "        best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        def check(addition: str) -> bool:\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompt = best_prompt + \", \" + addition\n",
        "            sim = self.similarity(image_features, prompt)\n",
        "            if sim > best_sim:\n",
        "                best_sim = sim\n",
        "                best_prompt = prompt\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        def check_multi_batch(opts: List[str]):\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompts = []\n",
        "            for i in range(2**len(opts)):\n",
        "                prompt = best_prompt\n",
        "                for bit in range(len(opts)):\n",
        "                    if i & (1 << bit):\n",
        "                        prompt += \", \" + opts[bit]\n",
        "                prompts.append(prompt)\n",
        "\n",
        "            t = LabelTable(prompts, None, self.clip_model, self.tokenize, self.config)\n",
        "            best_prompt = t.rank(image_features, 1)[0]\n",
        "            best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if clip_use_artist:\n",
        "            batch.append(self.artists.rank(image_features,1)[0])\n",
        "        if clip_use_flavor:\n",
        "                best_flavors = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "                extended_flavors = set(best_flavors)\n",
        "                for _ in tqdm(range(clip_max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "                    best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "                    flave = best[len(best_prompt) + 2:]\n",
        "                    if not check(flave):\n",
        "                        break\n",
        "                    if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "                        break\n",
        "                    extended_flavors.remove(flave)\n",
        "        if clip_use_medium:\n",
        "            batch.append(self.mediums.rank(image_features, 1)[0])\n",
        "        if clip_use_trending:\n",
        "            batch.append(self.trendings.rank(image_features, 1)[0])\n",
        "        if clip_use_movement:\n",
        "            batch.append(self.movements.rank(image_features, 1)[0])\n",
        "\n",
        "        check_multi_batch(batch)\n",
        "        tags = best_prompt.split(\",\")\n",
        "\n",
        "        return tags\n",
        "        # check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n",
        "\n",
        "        # extended_flavors = set(flaves)\n",
        "        # for _ in tqdm(range(max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "        #     best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "        #     flave = best[len(best_prompt)+2:]\n",
        "        #     if not check(flave):\n",
        "        #         break\n",
        "        #     if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "        #         break\n",
        "        #     extended_flavors.remove(flave)\n",
        "\n",
        "        # return best_prompt\n",
        "\n",
        "    def rank_top(self, image_features: torch.Tensor, text_array: List[str]) -> str:\n",
        "        text_tokens = self.tokenize([text for text in text_array]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return text_array[similarity.argmax().item()]\n",
        "\n",
        "    def similarity(self, image_features: torch.Tensor, text: str) -> float:\n",
        "        text_tokens = self.tokenize([text]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return similarity[0][0].item()\n",
        "\n",
        "\n",
        "class LabelTable():\n",
        "    def __init__(self, labels:List[str], desc:str, clip_model, tokenize, config: Config):\n",
        "        self.chunk_size = config.chunk_size\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.embeds = []\n",
        "        self.labels = labels\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
        "\n",
        "        cache_filepath = None\n",
        "        if config.cache_path is not None and desc is not None:\n",
        "            os.makedirs(config.cache_path, exist_ok=True)\n",
        "            sanitized_name = clip_model_name.replace('/', '_').replace('@', '_')\n",
        "            cache_filepath = os.path.join(config.cache_path, f\"{sanitized_name}_{desc}.pkl\")\n",
        "            if desc is not None and os.path.exists(cache_filepath):\n",
        "                with open(cache_filepath, 'rb') as f:\n",
        "                    try:\n",
        "                        data = pickle.load(f)\n",
        "                        if data.get('hash') == hash:\n",
        "                            self.labels = data['labels']\n",
        "                            self.embeds = data['embeds']\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading cached table {desc}: {e}\")\n",
        "\n",
        "        if len(self.labels) != len(self.embeds):\n",
        "            self.embeds = []\n",
        "            chunks = np.array_split(self.labels, max(1, len(self.labels)/config.chunk_size))\n",
        "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None, disable=self.config.quiet):\n",
        "                text_tokens = self.tokenize(chunk).to(self.device)\n",
        "                with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                    text_features = clip_model.encode_text(text_tokens)\n",
        "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "                    text_features = text_features.half().cpu().numpy()\n",
        "                for i in range(text_features.shape[0]):\n",
        "                    self.embeds.append(text_features[i])\n",
        "\n",
        "            if cache_filepath is not None:\n",
        "                with open(cache_filepath, 'wb') as f:\n",
        "                    pickle.dump({\n",
        "                        \"labels\": self.labels, \n",
        "                        \"embeds\": self.embeds, \n",
        "                        \"hash\": hash, \n",
        "                        \"model\": clip_model_name\n",
        "                    }, f)\n",
        "\n",
        "        if self.device == 'cpu' or self.device == torch.device('cpu'):\n",
        "            self.embeds = [e.astype(np.float32) for e in self.embeds]\n",
        "    \n",
        "    def _rank(self, image_features: torch.Tensor, text_embeds: torch.Tensor, top_count: int=1) -> str:\n",
        "        top_count = min(top_count, len(text_embeds))\n",
        "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).to(self.device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            similarity = image_features @ text_embeds.T\n",
        "        _, top_labels = similarity.float().cpu().topk(top_count, dim=-1)\n",
        "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
        "\n",
        "    def rank(self, image_features: torch.Tensor, top_count: int=1) -> List[str]:\n",
        "        if len(self.labels) <= self.chunk_size:\n",
        "            tops = self._rank(image_features, self.embeds, top_count=top_count)\n",
        "            return [self.labels[i] for i in tops]\n",
        "\n",
        "        num_chunks = int(math.ceil(len(self.labels)/self.chunk_size))\n",
        "        keep_per_chunk = int(self.chunk_size / num_chunks)\n",
        "\n",
        "        top_labels, top_embeds = [], []\n",
        "        for chunk_idx in tqdm(range(num_chunks), disable=self.config.quiet):\n",
        "            start = chunk_idx*self.chunk_size\n",
        "            stop = min(start+self.chunk_size, len(self.embeds))\n",
        "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n",
        "            top_labels.extend([self.labels[start+i] for i in tops])\n",
        "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
        "\n",
        "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
        "        return [top_labels[i] for i in tops]\n",
        "\n",
        "\n",
        "def _load_list(data_path: str, filename: str) -> List[str]:\n",
        "    with open(os.path.join(data_path, filename), 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def _merge_tables(tables: List[LabelTable], config: Config) -> LabelTable:\n",
        "    m = LabelTable([], None, None, None, config)\n",
        "    for table in tables:\n",
        "        m.labels.extend(table.labels)\n",
        "        m.embeds.extend(table.embeds)\n",
        "    return m\n",
        "\n",
        "def _prompt_at_max_len(text: str, tokenize) -> bool:\n",
        "    tokens = tokenize([text])\n",
        "    return tokens[0][-1] != 0\n",
        "\n",
        "def _truncate_to_fit(text: str, tokenize) -> str:\n",
        "    parts = text.split(', ')\n",
        "    new_text = parts[0]\n",
        "    for part in parts[1:]:\n",
        "        if _prompt_at_max_len(new_text + part, tokenize):\n",
        "            break\n",
        "        new_text += ', ' + part\n",
        "    return new_text\n",
        "\n",
        "ci:Interrogator = None\n",
        "\n",
        "        \n",
        "def process_img(img_path):\n",
        "\n",
        "    display(HTML(f'<div>-----</div><h1>{img_path}</h1>'))\n",
        "    # Load image\n",
        "    with Image.open(img_path).convert('RGB') as img:\n",
        "      display(img.resize((200,200)))\n",
        "      # Get existing caption\n",
        "      existing_caption = ''\n",
        "      cap_file = os.path.join(folder_path,os.path.splitext(os.path.split(img_path)[1])[0] + '.txt')\n",
        "      if os.path.isfile(cap_file):\n",
        "        with open(cap_file) as f:\n",
        "          existing_caption = f.read()\n",
        "\n",
        "      # Get caption from filename if empty\n",
        "      if existing_caption == '' and use_filename:\n",
        "          path = os.path.split(img_path)[1]\n",
        "          path = os.path.splitext(path)[0]\n",
        "          existing_caption = ''.join(c for c in path if c.isalpha() or c in [\" \", \",\"])\n",
        "      \n",
        "      \n",
        "      # Create tag list\n",
        "      out_tags = []\n",
        "      new_caption = ''\n",
        "    \n",
        "      # 1st caption pass: GIT\n",
        "      if git_pass:\n",
        "          new_caption = git_caption(img)\n",
        "          # Check if caption fails from list of not-allowed phrases\n",
        "          if second_pass and any(f in new_caption for f in git_fail_phrases.split(',')):\n",
        "              # Fail git caption\n",
        "              print(f'GIT caption was\\n{new_caption}\\nGIT Fail phrases detected. Beginning second pass.')\n",
        "              new_caption = ''\n",
        "\n",
        "\n",
        "      # 2nd caption pass: (if failed)\n",
        "      if second_pass and new_caption == '':\n",
        "        if second_pass_type == 'BLIP':\n",
        "          new_caption = ci.generate_blip_caption(img)\n",
        "        else:\n",
        "          new_caption = coca_caption(img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Add enabled CLIP flavors to tag list\n",
        "      if clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_movement or clip_use_trending:\n",
        "          tags = ci.interrogate(new_caption,img)\n",
        "          for tag in tags:\n",
        "              out_tags.append(tag)\n",
        "      else:\n",
        "          for tag in new_caption.split(\",\"):\n",
        "              out_tags.append(tag)\n",
        "\n",
        "\n",
        "      # Add parent folder to tag list if enabled\n",
        "      if folder_tag:\n",
        "          folder_tags = get_parent_folder(img_path,folder_tag_levels)\n",
        "          for tag in folder_tags:\n",
        "              out_tags.append(tag)\n",
        "\n",
        "      # Remove duplicates, filter dumb stuff\n",
        "      # chars_to_strip = [\"_\\\\(\"]\n",
        "      unique_tags = []\n",
        "      tags_to_ignore = []\n",
        "      if ignore_tags != \"\" and ignore_tags is not None:\n",
        "          si_tags = ignore_tags.split(\",\")\n",
        "          for tag in si_tags:\n",
        "              tags_to_ignore.append(tag.strip)\n",
        "\n",
        "      if uniquify_tags:\n",
        "          for tag in out_tags:\n",
        "              if not tag in unique_tags and not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                  unique_tags.append(tag.strip())\n",
        "      else:\n",
        "          for tag in out_tags:\n",
        "              if not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                  unique_tags.append(tag.strip())\n",
        "\n",
        "      existing_tags = existing_caption.split(\",\")\n",
        "\n",
        "      # APPEND/PREPEND/OVERWRITE existing caption based on options\n",
        "      if existing == \"prepend\" and len(existing_tags):\n",
        "          new_tags = existing_tags\n",
        "          for tag in unique_tags:\n",
        "              if not tag in new_tags or not uniquify_tags:\n",
        "                  new_tags.append(tag)\n",
        "          unique_tags = new_tags\n",
        "\n",
        "      if existing == 'append' and len(existing_tags):\n",
        "          for tag in existing_tags:\n",
        "              if not tag in unique_tags or not uniquify_tags:\n",
        "                  unique_tags.append(tag)\n",
        "\n",
        "      if existing == 'copy' and existing_caption:\n",
        "          for tag in existing_tags:\n",
        "              unique_tags.append(tag.strip())\n",
        "\n",
        "      if append_tags != '':\n",
        "          for tag in append_tags.split(','):\n",
        "              unique_tags.append(tag.strip())\n",
        "      \n",
        "      if prepend_tags != '':\n",
        "          for tag in prepend_tags.split(','):\n",
        "              unique_tags.insert(0,tag.strip())\n",
        "\n",
        "\n",
        "      # Construct new caption from tag list\n",
        "      caption_txt = \", \".join(unique_tags)\n",
        "\n",
        "      if replace_class and sub_name is not None and sub_class is not None:\n",
        "          # Find and replace \"a SUBJECT CLASS\" in caption_txt with subject name\n",
        "          if f\"a {sub_class}\" in caption_txt:\n",
        "              caption_txt = caption_txt.replace(f\"a {sub_class}\", sub_name)\n",
        "\n",
        "          if sub_class in caption_txt:\n",
        "              caption_txt = caption_txt.replace(sub_class, sub_name)\n",
        "\n",
        "      tags = caption_txt.split(\" \")\n",
        "      if cap_length != 0 and len(tags) > cap_length:\n",
        "              tags = tags[0:cap_length]\n",
        "              tags[-1] = tags[-1].rstrip(\",\")\n",
        "      caption_txt = \" \".join(tags)\n",
        "\n",
        "      # Write caption file\n",
        "      if write_to_file:\n",
        "          with open(os.path.join(output_path,cap_file), \"w\", encoding=\"utf8\") as file:\n",
        "                      file.write(caption_txt)\n",
        "                      print(f'Wrote {cap_file}')\n",
        "\n",
        "      display(HTML(f'<h2>Final Caption</h2><div>{caption_txt}</div>'))\n",
        "\n",
        "\n",
        "\n",
        "# processor = None\n",
        "# model = None\n",
        "# coca_processor = None\n",
        "# coca_model = None\n",
        "# ci = None\n",
        "# with torch.no_grad():\n",
        "#     torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "\n",
        "try:\n",
        "  if git_pass:\n",
        "    if not git_loaded or processor is None or  model is None:\n",
        "      print('Loading GIT model...')\n",
        "      processor = AutoProcessor.from_pretrained(git_model_name)\n",
        "      model = AutoModelForCausalLM.from_pretrained(git_model_name)\n",
        "      model.to(device)\n",
        "      git_loaded = True\n",
        "  else:\n",
        "    if git_loaded:\n",
        "      git_loaded = False\n",
        "      processor = None\n",
        "      model = None\n",
        "      gc.collect()\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "  if second_pass and second_pass_type == 'Coca':\n",
        "    if not coca_loaded or coca_model is None or coca_processor is None:\n",
        "      print('Loading Coca model...')\n",
        "      coca_model, _, coca_processor = open_clip.create_model_and_transforms(\n",
        "        model_name=\"coca_ViT-L-14\",\n",
        "        pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
        "      )\n",
        "      coca_model.to(device)\n",
        "      coca_loaded = True\n",
        "    elif not coca_loaded:\n",
        "      if coca_loaded:\n",
        "        coca_loaded = False\n",
        "        coca_model = None\n",
        "        coca_processor = None\n",
        "        _ = None\n",
        "        gc.collect()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "  else:\n",
        "      if coca_loaded:\n",
        "        coca_loaded = False\n",
        "        coca_model = None\n",
        "        coca_processor = None\n",
        "        _ = None\n",
        "        gc.collect()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "  if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending or (second_pass and second_pass_type == 'BLIP'):\n",
        "      # if (not clip_loaded and (clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending)) or (not blip_loaded and (second_pass and second_pass_type == 'BLIP')):\n",
        "        # print(f'clip_loaded:{clip_loaded}\\nclip_user_movement:{clip_use_movement}\\nclip_use_artist:{clip_use_artist}\\nclip_use_medium:{clip_use_medium}\\nclip_use_trending:{clip_use_trending}\\nblip_loaded:{blip_loaded}\\nsecond_pass:{second_pass}\\nsecond_pass_type:{second_pass_type}')\n",
        "        ci = None\n",
        "        gc.collect()\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "        cfg = Config(quiet=False)\n",
        "        cfg.clip_model = _clip_model\n",
        "        cfg.clip_preprocess = _clip_preprocess\n",
        "        cfg.blip_model = _blip_model\n",
        "        ci = Interrogator(cfg)\n",
        "\n",
        "\n",
        "  for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "    for name in files:\n",
        "      if 'txt' not in os.path.splitext(os.path.split(name)[1])[1]:\n",
        "        cap_file = os.path.join(folder_path,os.path.splitext(os.path.split(name)[1])[0] + '.txt')\n",
        "        if not existing == 'skip' or not os.path.exists(cap_file):\n",
        "          process_img(os.path.join(root, name))\n",
        "        else:\n",
        "          print(f'Caption file {cap_file} exists. Skipping.')\n",
        "\n",
        "  \n",
        "# except Exception as e:\n",
        "#   print(f'Exception occurred: {e}')\n",
        "\n",
        "finally:\n",
        "  # processor = None\n",
        "  # model = None\n",
        "\n",
        "  # coca_processor = None\n",
        "  # coca_model = None\n",
        "  # ci = None\n",
        "  # with torch.no_grad():\n",
        "  #     torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "   "
      ],
      "metadata": {
        "cellView": "form",
        "id": "7MYkepUPzgSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F06v8KVh7m7j"
      },
      "outputs": [],
      "source": [
        "#@title Define your Huggingface Repo\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model/dataset repo doesn't exist, it will automatically create your repo.\n",
        "dataset_name = \"\" #@param{type:\"string\"}\n",
        "make_this_dataset_private = True #@param{type:\"boolean\"}\n",
        "\n",
        "datasets_repo = user['name']+\"/\"+dataset_name.strip()\n",
        "\n",
        "if dataset_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(datasets_repo)\n",
        "      api.create_repo(repo_id=datasets_repo,\n",
        "                      repo_type=\"dataset\",\n",
        "                      private=make_this_dataset_private)\n",
        "      print(\"Dataset Repo didn't exists, creating repo\")\n",
        "      print(\"Dataset Repo\",datasets_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Dataset repo: {datasets_repo} exists, skipping create repo\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dUKhs0SB60VE"
      },
      "outputs": [],
      "source": [
        "#@title Upload Dataset\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary\n",
        "dataset_path = \"/content/dataset\" #@param {type :\"string\"}\n",
        "dataset_name = \"dataset\" #@param {type :\"string\"}\n",
        "#@markdown #### Delete zip after upload\n",
        "delete_zip = True #@param {type :\"boolean\"}\n",
        "\n",
        "tmp_dataset = \"/content/dataset\"\n",
        "\n",
        "dataset_zip = f\"/content/{dataset_name}.zip\"\n",
        "\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload captioned dataset\"\n",
        "\n",
        "def upload_dataset(dataset_paths, is_zip : bool):\n",
        "  path_obj = Path(dataset_paths)\n",
        "  dataset_name = path_obj.parts[-1]\n",
        "\n",
        "  if is_zip:\n",
        "    print(f\"Uploading dataset to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/blob/main/\"+dataset_name+\"\\n\")\n",
        "  else:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/tree/main/\"+dataset_name+\"\\n\")\n",
        "  \n",
        "def zip_file(tmp,zip):\n",
        "    zipfiles = zip \n",
        "    with zipfile.ZipFile(zipfiles, 'w') as zip:\n",
        "      for tmp, dirs, files in os.walk(tmp):\n",
        "          for file in files:\n",
        "              zip.write(os.path.join(tmp, file))\n",
        "\n",
        "def upload():\n",
        "  zip_file(tmp_dataset,dataset_zip)\n",
        "  upload_dataset(dataset_zip, True)\n",
        "  if delete_zip:\n",
        "    os.remove(dataset_zip)\n",
        "\n",
        "upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<a href=\"https://www.buymeacoffee.com/theovercomer8\" target=\"_blank\">![blunt2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASEAAABQCAIAAADZdlScAAABbWlDQ1BpY2MAACiRdZK7S8NQFMZ/topvHFQQceig4lBBFMRR69ClSKkVfC1JbFohqSFJEXEVXBwEB9HF1+B/oKvgqiAIiiDi5u5rkRLPtUJF2htuzo8v9zucfAmEEpZhe7XjYOd9NxWPRWbn5iP1L4RoBNrp1AzPmUgmE1Rdn3fUqHo7qHpVP1dxNS9lPANqGoRHDcf1hWUaEqu+o3hLuMPIaUvCh8JRVwYUvlK6XuJnxdkSvyt206lJCKmekewf1v+wkXNt4QHhXtsqGL/zqDdpyeRnpqV2y+7BI0WcGBF0Cixj4TMoNS+ZVfYN/fimWBGPIXeHNVxxZMmJNypqQbpmpJqiZ+SyWFO5/8/TM0eGS91bYlD3FARvfVC/A8XtIPg6CoLiMYQf4SJf9q9ITmMfom+Xtd4DaNuAs8uypu/C+SZ0PTiaq/1IYdkh04TXU2idkx/gBpoWSln9PufkHtLr8omuYW8f+uV82+I3ocln3jPEA2kAAAAJcEhZcwAAFiQAABYkAZsVxhQAACAASURBVHhe7X0JtBXVme6/q+rMdzp3ZpRREIGLICCgCCo4RDHi2G3m7szprF5ZK/36pZNeyXovq19Wr5eV7sSkk5iOnZiXpGNMFI1DouI8gQrIIAgIXLjceThzTft9f1Wd4U7cy3AviPXlBOvUrdq1a+//+6f9Vx0xe/Zs8uHDx5hBG+mAIqS0SVrOv/ZIx44SAv8rQsphD/Th46xDCP6/UPFxtkeLkTkmpcWkEkq4bGqkcla4bEowXCvU4EjnlYDJwx/pbvB/vA20LL0NqQbLcJWR2vLh4+zANtJ6pjWTOJzu3Wtk2iHADtlGltgROAaCBcLVlY2raiavr6hfooXiihpUFKKToHEJhjFUrgEbRW99+Dh7kGRb+ORy6Zbe4y92HHki2f6mZWbESIIrhovHwC6hBKomXD5x7t9WNixVNZWdRBqWJz58fCDgxDeglZFLdx158uien6e63nZoNqzZUWtqagbvlbapBmJTFvzd9MVfi8WnEylnLATz4eP9D3heKhhSPTc+4UrLSKW6dyEAEsMEaUNxTNpauGrG0m9NuPAjihrw2eXDx9CQFAiVxydeCcokOt5g5g1Fs4Eck9JWA2UzlnyjYdYtvlvow8eIUBS1on6pbWYS7W84DuNAmg22Y9bkeZ+eeNGnyIcPH6ODUJTy6qZ037uZ3n2cbOyPfikRhGHldZdOmPNxtni+EfPhY5SQpIXLpyz4u1BsMq919Uc/jilqaOKcjwWjNf5qsA8fJwVpU3nt/LppG+iEHJNldYurJ17lE8yHj1ND7fSbQ7FJAwqhihyT0q6esFoLRX0v0YePUwCMU7RyRlntIts2S/fnOSbtQChe0bjiFAs4fPjwwYtmWnzCFYoaKM1neBxDoBaMNoaijb6j6MPHKQPsiVTOVrSolENwzNaCcS0U9znmw8cpA/QJxyaqangIO4ZdajCmBgLnfjDmFfH78HEOQpIWqhFKv6dSCvEYOQ/GDHHWOQVpU7DsWLD80EgH+vBxdiAUdUBS433zPAnYpah61YxHJ6z4Urj2Fb+K0sf7BSM/o3kuAIwKVTZXL/i/4bqnbCttmHeNdIYPH+cK8hwTJM+ZdIfz1HW/PYFYZ+3ibwaqX8iko4pUDD0wzKk+fJxzKJFlflfH8AeOG4Qdrd1VWjCJzei0X8uKJzKpMlMXyUTESDec85GjDx8eSuo8bPPsBzmShJquXfR/6hZ+Twkm3BSiEuwVFa/mcsF0xjStTDbZQLkpp79WLsl/SY+P8UBJXlHRznpeETKvhroCwc6aC3/YuPTrgdgx0MCSpiQ7kxambplmxuhtEsak0+SYbVNlmayv8VcBxhCGyZ8RYVmUM8gcWEl7/qBox5hjZyXLKGyhGp4JZTvWa5u2nauONT4eX/htJZSQViCTVrJpyzCzuUzE6rqBbMV9mRUVPicDy6ZpE+kbfyP+/i4Kajy7mGbsBPF8y3amgMGcP5MunTfCYRj2KY20ZjHNnHLeDv7ZziuCVIHu6ot+mj1+Q/L4QhhS2wpalmZYIqeXheufrJg5r2f3F/VEfTbaEw1Okc1fVRNr2dwFUkqwz6acFBlVsc30LLJCI12MYTtkvn4V1cUpkabpk2RnL2mqMC2Z00UixapX007TTPqgm1bThtUcTP/ve+m9Y6QOfHCRgXm8bD7dfQNVlVF7N33zJ4TxV8ZM0duSTEyuOoaXGBJnn2NK8KhW+Vj9BU8Hd321a986K9vY06eVBRKJlBqJaaGJv1cP3pHrXWCKGdnm/xHquYMi71RO/Uu4em9O2Z21eyOhBCWu7dn2nZGuxCoTqCyjZRfTgpm8XRahr9wtVIUUQemsANmOtNHug/TmHupLk6ac1JsqfXgwLFo2jzau9Xg1uYH2Nw/BMbgPE+voIx+iyhi1dNCEWmqsod7kWK3YwlVBaABKb91DR9tIHaPLDIWzzTFoumC7pGQ60xGb9S0RaOl+5yN9x5bntNctI5bNKoFgiww2B7LLtY6Z1LfKqvl++bTfBqq7Ehkll5Hx8nAwVN6+cyOZwRNPDswXZnH9ZbR0HtPMBUzWO4cooFFFjCd4Wox9yMub6Fg7PfQcvfY2K1qfZScF2IqKKN28hkkFNwFGo65qaF8eO5fPZ4JB9z2zle5cR5FROSKnAnihmPTP30azJpNuykMt4oPEMYiw1iplRuYiPXpf5bR7Mr0VyY5bk9FH7eA+2ygLRRCtHQ+GNavnEj38ZHjiz3SR6TlO0SA1xiO2km1rXmp2LzpxJIlZnDmZPrORJtZSKkM7D9DFM3g/jNWPH6BUlicAUcFVS6lpNovFpHr69Ieptkpuem5cJ+M8AMzFkotoaiNvv7SNLryAwqGhA2YM7OK5vAH3oaOHBSGZGSuNBgFYMJsJBoQC4mQD+NPEOErQMDdmWyHDllB7hiGy2Zza8G/SzqqHf2T3rTGt3mQiY5lZJdRtiXbZ+N2c0p1I2XXl4Qm1ZYZttbXLzJHbhVU+dNNu+5J9wo/ewATDWP98E23emv+TTbaj5LoT9NY79P3f0D2/o+4+/hOM24bV4pI553O+aywQCtDSi9n3xrDv2M+cSaWHcAbw15pKGa/gbRAsGuYYKZmRY0QyTPSkOm9b10946Bhg3DgmKdBGIjeYaaYRS6ctwzYh7OkUaZF2Wfn/1Nz8cMs9ateXrfR0LahLW5OVTwSqdoSUcEM8BuPS3ps5fDzXd+CLwcSHTjwxGF/4gTMdHdbaRVt3FX0SzhrrPLvQqSAVuvbKDvrPhymT47+Gg7Tm0nF13N/vgLaqi8sLp/J2ew+zKxRkZ0EZzDEbwZiIOhOBGKyxmnoSlMmOVfwL36S6kjcwxanceIfZ4+critqfBaO9dtctRt8iaYacH8Hg/TI3Jdldq6oduq4YfYqIB8zA66rWrNiTY73fCATWRxp3WlZU0eeofbfZwZbOTLeRi5h6ebDvjlhqw2g0X2Xezu09TFm9SBvEYzBTha/CUcNv7KG39tKKBbwHPk8wQLox3rPyPgXchAsvEK4KO3KcwmEOdEGhwaOHI2sqeWyBTIYumEDNbTw1YzHOsJmRMFU5MoDpRrAwFlc5AcaJY9AfWqAjWP2UWvcKpRalW9cZPSulHodcK8asbGpmJnaIZJmeM3t7DMNMadSjEJueMM3VYk9l931emHG791qpZBVKh2VIUEjYwdEQDGbqQDM9/yZvPP4S79HyOa7MMG7Dzv0ex7SSbBiUNLSvMijZaDsrdaXmzj1SKAP1t+38YMBoDKPttKCWXEvKgb8kRa73PfSraUuOcRyHU5CqwS+9HTEDhFPcSAxo7aTqcl4FOdI6hB3Djqp85ika4QAYwRt8Cjf9OORASWc/ulTamrukWZq0HDAmXMNgwZzKqnLeZZqUzvB+7MSRaGoc8vjjxDFiWZcC3LCNaO1bMvZyJDMv+d7nrN7lYIuauiGZ2BwK61KKbFqxYTmsZFDwCIYaNuvdC8hgNkpbITuqUNSzgaOTG4wjVOlP/sCHS/I8Qxe5oTgmSiSyrYttnXB6AqVbW8VNpbPFA6CPy6NQk7Kzx9tlO3xrrOW57Ev1OxJKPaDJnsQI/caRsTAhVuns5QII4ewJqGQ68lSQCXehj+NYs58ucCEdecLBQY1Px7Y66kU/NwSFSUfLuB007l5LUU5EM/QNFmxCrfe1q4+XlY938sIXy73dT5rBPdewQNYRKcGl3HXQ4YZzs9WVEt1u7y5eyiV8Q7VMZ0XBEOFIEBUUxSXcHvK/gsef79cZE4QAleWyPi7izuVwR10JqaliUr2cOVm82yyPtonBKuDMYpw4Jlj4MFeWZWvJlG7bIhTdGZz6tdzRz9ldt4eyt5kdB9OV/yGEkJbQjVxIlRKiE2vRKt/I7PkKD1zkeKDsSCDcaelVud7ZVrZmtCJDReWnONOg5W8aM+HWi9h5Ze8G61MavAO27WWBw6n1NXT7NbRkLj3wFD3yPM8cOXM8dQJ98kbQTPzLfdTVy02BSBuvoisuoRfepJ89XDxy4Sy69WoQVXz7Xg4JhptXXG7WVLr9arpoGv3wAXp5O/dq2cWc5u5L0hOvMO0hrJCVCXV03Qoqi/Iaw9bd/awcBDcWocULafZUTpnCXL97mF54a2Snl8ms0bKL6OLpVFFGyTS9+jbteJevdf1KVhmbnuPGh2yEc0tRGa/gv0nnRuZcwFHW4rnMUpDtaFtRR0BluA48vsKItXTQvsMOhyWtaqIPrxFg+D/dQ8lsfuIU7sD1K8VfXqMHnyaFRYkuvZg+fCWz+uv/Qcfa+DBM3PoVTPUtu+jVndyPyxfRLWshUV4QjtaWzRPTJtIlcwTu9EcPiMPHubUxxThxDCIgjSrTsqRtmTlDURTTELpspbrvWLm4mrqmLPM/u01Lj/1QEaawGlW7QWh2ZMpvMy3XCBkpn/PjyqkPB0NpqWSkHcok6hOHNqYO30pytD81KEo2QvmTIAeQjKAz3yrntViOG2pl04V8+LF2enkHcxJB/KdvYVEgx2643hfEEXu+eBsvpMJkQSt39FBDDX32Fk5Y85GaR138u3QefeYWzmLDcsaivFowpILAtZZcRJ/c4PlREGUI9OrF9KmbOQFDDtW/+ys+DCYCDU52utQ0my3e/mbPmlkOS+++nmZO4hYgXrDAqxayLP76Cc/dGhK4o7q4vPt6sWh20fu6bAFt3kLzZ7H4gifY7u4bpgWJGxSw6uTk7jAsMB2NNawspOMyvLaTNVTCse3gGJQROfeIDwJgjEk0RDdfSR+6nBmVSBdtJohx42pQhb/WxtkJx8Z1K1nr4daYt07nZ02mL9zuGVKMCei96wD7HVBDofyzUNi+Yx3P1Ou7OXzYfZB7MtYYL45JMvuWU+VDGBTIXE7P2pbtvBk8kY3dE83NF2ZjpfGVRFbNhn5AZqVqTw5PedDSA5SZU7Xw27EJm3Tdzqa1bM6UMpvNthplb5lVnYGuL43emnkQxRGH/EEcN66l6gqerUyO3j2CbdFQzX+CQLR18nyvWeIRDLOOmXOvyHyYy5LE7ViUzvGP4yCKcwkGsCw6G9JmQxR29KhhFSP7AQEPv24lxKJTCFTQwsR6uutaj2DABY3cefD5cxu9SwPQ0HVx7jk5BMMpn93Iwo1buP8xOniMvnwXLwSjD4+/TO1dnpoYHFVGw/SJmwTsLTnJIVgMeHHrL+O6MxemNbDDpV9tx1csczgGMwuJx1dQZds+mj6R1QGGEd277xFSBbsSLhsBDDusJewSrOXVyzxDh3F264lBnngVmyP3UlBnaKS2hm5Y5aVMsCery4qY+PhNRU8Vl0bnwbHt++hff8FVXQudn9lr72FFc+AodXTzV44aBkW5ZxzjxTFMSXZmoiumRJpVNWiZlmnbAUWBwrOU7VZwp2Y2KhSrzH5NNWbYMhupf0OJbckc/mRszjfS2jM9RyKmYQs4kGQQBzyaULJq9B3ZeSrRfMGOQfHfvJp13pt7odLkumXimmW8H8L02z/zxHMoIile6R0PHXzwqFQd98UJD7z98Kk6u/nXp6rzR6KF3e954gJmhvKrBa2dlMl6cblr1grAhcClsoj3tS9N3Qm58SoB/kOqXMOCZstjEkwAwSy7X4LUBQ6Dymh0fibkTy8yTzBkxzuYY2jcdVxxIRAVTZXW4FomrVxKLsH2vEf3/DeLI9qHpv/iHR4fQJhCkhDnBgc1wstczl8jYV7Tx5Hw2X78IFfPfPWjHM2uWEib35D7jwiKoE3PUIEGcNhwMO69MDW4rm66IQa7u9H8ADa38c2WDlRzOyf9P7zWW2IuwHCiSjQLX3f+TI9j7JtsYxejEMFiTEbzcMDpYOyzKi4Q6ljT1NQGYUHwNClVC3Ypq2ezpmEYuthdOLDMuKuiolJp+F7y8HWZsnu7Uo93tyvppA67F9DUsBY1LTuX1ZM9Uatz4ykQjKhoFmBSjrTyBsKPtUtERd6AoItTG1lQXBQm/lgHHW0vVn4UGLX3kGOdqGghEZtxIaxzpCKKDgkiezeNcetV7OyFAv1kVCiy4KTt2g+rJVYuYH/1sZe8nfjrHdeIedPplbfp33/DQu8ix5qHP1Ma5RKneAKx09NboA5Yo7vZhbZuVvm4XLxcwvmsqfJcWRfQI+suc5rSWb909XmcfOcQLyq6ONzCfxX8yDzVV8vP3oqWpVXyzGEkP1Cqk2sBwK6aSh6Krc4Mg4QTa9mfhMRHwzx5OB036CaWYNzcEUP70FCm6V0LsaU7F7ipQy1e2FwYqLf381XWL+fynf96xNuJZnN5fwHNNuR/mwjhAL5qzlWgIJbNpzvXs3CWzsIZx3hxjCG0zN3Z9JRUuhfsymXNTCaby+UsXfSamyU5cwUbFW0ONjycOXIz6dMoMz+XaLKkyiNmJDs7Wtvaj6b6Ur2dFaL1nwLZNadm5sN5pYiRffQF+q9H6cVtHHa/8Cbd+xDrVEzhlYs5fe9m2LT8IMEK2SUiVeDk9ne9jcLEQ6ANw7tEIOB5NQCkLZvjIOem1ewKZoZfD4WzdNVS6knSrx/nQMgFIpzl8+ml7fTTB1k7uN2DPOkOw2HuFs4SbkYHHlpvQkB2EeC5Zg17uhPMw2UXs0NY+moJPnE218EAW3azK1VIC0E7lOctBlpwLTMG4eqlAs3m9H6/HRkoeQWE2/yMSWwGYUZgQNw91yynf/wk17W5J+IWEB0peWXkIp3j21fyfAPbXe6B7bBjpUlU9AROxC1XcY3Izzd5DrPbrJv+5WxnsJjEghPheBFes4j98NUyT8UbGj3Gz1cEVKs+nPhyIvIPNqVtW5iOjGSzhtS7ScBgB9VgX2TGdxJtq0XPX3HgZEwLpW6ztSNGaKshDykW7H+PZs0pN68O2EP/jPVoULBjGFkotidf9p53gLCCABj0T23gr3BswD1ENQXmuE48uQmPOhkv45nBbLnlV7KEjZAby8kjQxnPmOwV8kDioYlnTZV3Xw+GiD9spsHaszDXi+awjwTFDIItX1A8AJ7PLx5hAULs4UobiJrNJwyn5Jen9h9hXwsh0F3r2RzBfj7+Eh9THmN7BUnt6C7mrK2SUiNoAcPwlAL2T5tE1VW8jbk63sk/x+oUsMsVTTBNArdTkHg0VlAlGBN4qtMnEUzuxTP4XyMvx7On8Cow6Aqeu3usfLVaIbmfSjvLHu5Owae4gK+OCVLcICpP0csvoakN9LOHBEzc0vzjahh2GHnhjPmlF3tahpzgzR1z7IcRw13/4RmyoNlpDDGuHHNoswH3n1R/ZGu7SCqIywzDBHlsTdcUNTzp31O91dT+0ZL38IcUfW4oN9d13FwXXp6e1gmV2DFMoZZf98Q2pARaHNMD+a4qY0sFXhU8QIQonvaXtLJJlDuZMfhprgDhU/AqOx2OoWXshL8Xc0wBiAHP7a71rPvv28SxzYB1LUR6hT0Qi617uLYLewrFX+jYr5+kRIZpD2vsHoxms07GBV7ZlHrvSNByZRNnRNG3596k3z8l+5ICfb+8ierj9PCzLJ1ueCYdb6rGIRLuLlGypocBWdXkObq4hLuMYXFCXFRE6fk3+qt/UYwPoble2CafeIVrPhddyJ5ewY18Zgs98gLHh02zvSsaTrOyRPfhjpIpL5VfG5fL54tCsy4KlQCYr5mT+AZf3s7eaSxvcg3mGGsEeKfXLC2OczrnbeDI9Zex87Jz/5inFseXYw7K5Iawsaozd0+v/DGJBImQaWWlIoINf8zQXvvYv0HsMeSGstcMPUzhQ0KEVTUqjUols1I1Z5M9xI/EnxSKBQSlsZAzZ5AqmIWc4+aBJLbFBCvYMTf+kRy9yFVN3sRjOnEkPnD6a+PekXBguOBYIG4pWiFIxi1rOcn2nw9zXiFY4lm57XIYk9+JPjz6vDRMtjaRkpRJ83GWGNOJ+wN5jsFngzWYN6Noji5olM1t4slXOC3+9rvkFts2Ouk4CNZb7/QbBNxjIYWgavnFCSfRd8mF3n7dFNK5TURiV10qtu+TB4/1fyhBUjE2w70IAUUDO7ypQlaUiZtX8zPR6CqiRHRsYm3xSNf9xk1dMrdwLV7TE46ZWrVQuKXDALvfXIrQL70Bl+GPz0rOUgteu/eOhB5PCwScC2bTRdOLByNIk07lx+K5bB5/+Sc2qgMn4kzjLHAMM6iKmnr1n2PW6i79l2l6MahO1GLvGbHfm0f/QdhVVvAFGXsmVPmsYh1LZzJCkTo8SU1RKn9hWzE1dWPIuEOzZ450nWFRyOaVBv0u3Do6d+kGdgbhEAL0AXUhmPtrLxO1VcVTXClZu5gm5OkP5812anw2rGbxhaxAhtAUPn9+lZ7d6tmQUkhHzgqxInxCCDH2QCCiedFJOCUOwlH/IJ5Lfvg/CDNgiMCfgq/5xMvi4ee8vJ/mJusVXhyHvCLmRGBW2gFIZNa9NUG1lZ66Qf+vvKQYc7qxHxq58Qo2Do+9LGB/Si2AJMfTdgDDGI1wOxi6rj6B4MrtKvze5lZeXy64hZyqkcwQMHBRns9cR2ryuZMb5FVLi7aS605sxI3yuhWicDqMWGsHLyijhwVNAW2IGBU3C6WGKA5jVZF3OtACvN8Pr+U18Vd3DP2A9pnFeOY8BqJMXTM5/NPJ2q/qYxspfp/edrmRrkmF/9Go/lKg5leBSHcgEqyuqSkvqwkp8XCgWiFd2G2i4l677nNG+T2kdtMpwQ2fyNGXpXClHN6R67TsO8zBWECTBYHg7IJBc6bRNctYXNzyfKhVSNusKRzNF4D5hkAsmEVXLKI9h5iuLnYegNs2cGWpAAhKQae+e7i4klbgmKuGyQ3ZQ95fU1nW+jdewZF9Qcrnz/KCQ3f5ASeBgSsWcAdc/7MANAL7UHDDcHdw7dw6klWLvIJAcigHts+fyTEebOOuAxQYJDsIolxgAKsrPNXD45A3Jm/slnq+OqwAy+JCJ5AhWKi/sbyCT7ia4AkMr7ffWUC/abWoz6+aoEsHjxWbiuXX3KAccQlME0K1Z7ZI2LrC8biRdct5/5Ovsq4ZTfnoaWLsrzA8OLgSSllgSSRQn9Xf0tMVncGP5CK/Yk8hF0qkcoZu2/wyArs8Hiqr0cqrQvH68vJ4Zai8JTTxB3LSZ2R4K50kMHmI/l95m1dgOEHsTCfmw10kwehfuYQ3jrTSU6+5pXqikIGrKmcufWoDq0P81WtQ8qMZf3MzMwS+kAuwArH+J27kZa6nX4e+5yY6HN8JKnboOlTp6P78Nzdkd1FQz4X0AJUkGEB1kOH6lbR9P9d5uYAjdNs1LLXQI/Cg7lzH9Q29Kbr/T8zDAQzHV7cWCZhzAV16EQeWd67noHTbPm9koEomN9InbmLmb3rOqwwc0Ag4lsl623DnmL0mx4R3X8esAzP//KooLPQVzopX0p3X0rQJ3joKOdUYXJK2lvXd5i2coXGhClq3jNZeWtQIhlPjW7idQkQHD3BFE6uVdw5xGra4DBPgsXL3v/jWEHWeY4Gz4CsOAJSsLp6XPdcH7FWK8kgut0c3clm2HqKsLAzZtC0JqilCNSxL1dSQFkpmTNLrRWKVYkyhkwHYcu0KZhFmGlIVDtDURpnJwSXjQoF1l/H7HhD/HD5OP3uIJRJKDnQqKN2P3cAnwt9DQNXW5c0QiPf52/mZ+cdfpAsmcekQAK8M54IbP/gdsyXirAXhlCPHi3mRgX0r4ZLMRynkSGHBgSyuHJQUjC+5iGs4YM0efJpa2mnZAlbSAO507jSul51Uz0lIyOUvHuVbG+ymIgZ7dae8fiUbDXT74zfyywLg94IVW3dTk+PC1cXpS3ew6P/mSSe5P0g60Z/OXjrcyiwFVjYxG3GzKxbyfe1vhn6R6RyHcLLEXYfQ//1d7NRt3cNy/9fX8k70/399gXMzh1rooWfpYzd6B69ezJqlvYdbw0yRo+DMEr1TcPyWzOUcI+but09SMlV88PnWq7lLMPu/eYJ10+ChGAuMy0VOCEu8Z8rdQf3rQXV6jf297sy/ptWH7GAioMb6EllpI3gWWT0b0qKKEujt61VEIGBeHkx+XtWbTnZ9DHJw0QxWmcCUel5rguTBdQwFRV0VSzM8mee287y2dhazdoeO83wDbvD9pxc5oJo+iTNsASeDAoJt2U0Pbqa/vs7jWKXj/f/hGdpzgL50pzeX+kjrMIUpx2FW3p9USh4UkCVpmkzeZsJModu/fIwf4QE5YSr/9mavquiCCfwhpzzi/sc4+TGkVAleNBd/RP+vZ6FH9IIPCPPAX1ihuF3WnApDBD+IJ4f0r/i9QxkeHDAEdgOWHO4rOabmyVd4f2evlyMRHOtK0xJu3hWjyt37E9cTu9WV6GRDNcdRuBcY/4J3EIswb3/5qEdjt1dGSYVXKr8oj6tDH/36cS4Kw18LDzHBMsMXwFBg//gQDFBrajhOl7YZjc+pm/6h8X5VMPSQ9oJh7I7Iz5FQNVEWo2s1OUeX7bbozundyVTatMxsNtfb261jrM3ZwdTnI+mvqNbkkyUYOWFARzd7FLympLPXlDMEO1Q6q8yXd9B//4Weed2pBijoackSgImHoYDrv+l5lhiIMrwR7IES7erjxPHv/syi4IbXMFwHjzLBEPl89laOYVzg9NfeHjbIhigkUtwgPC4wHG0mnCeIcS1YVEgGYgwoe/dxLBCv26nog1Ae62ALhuMVlaUfhuvtA5TL8QEQUNgcsALqHCeewDVCmziSS+OdpV6YlF89wWOSTHO+GwYBRvj5N+ihzd6T40MCtwBDCpWEjnX28cYbu7jsc/NWbqSUmRj2rl7uPJjM13oMQZdIOstimNa2bs583v84d4DvyKnqSmb5K8YZppUrhiV1JVitbNmVL6QUfO+4Efjnza30x2f5UQPFyfekHP8cUfGRNh6rl7bxLJy8+IwSsuWd+ywjIfIvmRGzZ/M6hW1ma6ffNG/tPfYY124NRjrwHanrMfpG4aY5JqakIfYnxaaU9aothSoUzZ4RCOTp7wAABc9JREFUo+uialNANpzO+pjr+EEjhgJcuISvwgm7U7zKxEM/WEm7p0SCTEvLLr59EZIEXeiGYa5Ktm1vnSfjPL3y2du44B0TX+eYQYjaTx48UaYY0qA7bbpxVGGN2E0PwmMsjcek4ybhWvCXnNxM8U+2k4ZBl7DT9crc7o0IdwXMfSbNtZ/SWc9FO/iKpkbTjttbt5rRTagOaffcRUUc5mbk3WOwE74cW56sVxhA5AXMUScT4zbo3iDnSAQrwUKX3CPRgvtS2oJOMZ2n3fmJOGvgWJ1xSGltfWhNLnXMKXlnjOXVRgc9nQqIeUIrekGSUzEIxZrCdlO10N3kiKJ6cczpEIzIkxuwIpUpNiTIm+/hTiEnHQ/FVJpMCzhLSaVz5sqEWyn3V9cyweBD7j3EPqS7/8TKUwivgtEoIRjlH8c2+7ua2HYT1kQDhQbdCDm3aVknJ0/uhdCBoteav83RN6UV1vSdbgwHt7VCuXNhp6tHSucC7eCjO4uWbuPeDcp+BCsc6bZQarTd1Yus4d3OOGPcLzgIZeILiogMWqlyKMdrQd4LBQYfcMpwGXWyvsKQT1UOVuqOPqB1S/kpFTiZ923yonMir254RAxnKIbcfwKrIuik79HF4DZHtF2DMcpTRn+t0znS3Tm6Hp15nH2OaVojnUeAtWm6kB8fhAv3+6c5tonms4XZcX/rmI9zAWdzfez8A3hVHuUyXMROr+zg98zB9yusIKfyqzo+PlDwOXYmgSBkzaX87BlCr6deZ8rBwywUqhbKIHyc1xjok559X/F8AozYyoW8cbiV3jvKyRKEAVX5tzvyG6fPVkzgY7xgmxnZfwXMt2NnEmVR6S7+dvZ6BcewY3VO9XBPkpLp08yJ+ngfwCFYvwydz7EzBolgLCbc1HB9nI0YaFZX7f1MzMFj/LTFcKu3Ps4bKGpA9H/m05/zMwbBRbHSXbCaVMd168CaJV614fa9J3qzgI/zBkIMfFmsH4+dSfQkxPZ9/PxfKEhfvJ2rmeZM5QHfuZ8LtcanytvHWcYgPepz7Ewip9P9j/HDyxfP5PopfFJZrq/742ZO3I/Do0o+zkH4HDuTAIs6usWPHuCXDtRUcklUexdnO2h0PyXh47xEsS5NWsZ4F92fj3CzGm2d/OZQ4dRV+ez6QEGaORomd6+YRtLSx+xX1j5gANO43lwd9rcjfJyXAH1yuXbb7lf9nX/ERShmrtPIdZ21wkkfPs4DgGPJo7aZpRIUOZZLH88kDvl2zIeP00Gq623LTJe+PznvKwrF0hO9Lc/LM/gMiQ8fHySAVpaR7Tn+grStIXxFPkJRu449m0u2+u6iDx+nAkGJjm3Jjh2K0u9p39Kcl8j07O04/KjvLvrwcdLgzLzdduD3Jv94Qz8K9csrS7Ja9t6f6jkg/HSzDx8nA0WhrmObO488QWLgmnM/MgmhZfsONu/4gZVL+R6jDx+jBOxWque9Izu+b+nF11EV4L37reRoJd33rpSysnGF4nuNPnyMBHDKyHYd2PKt3pYXhTLEe5cGcszJh8i+9i3SypXXLla0wJl8W40PH+cXQLBcqnX/a//cefhxRRn6FdCDOUZszKRMtG/NppojlbOD4WrfnvnwMQCCXy8re46/fOD1b/Yce05RtMFvGXAxJMe8N2Wlunf3tLwopR0um6QFY97vFw5xtA8fHwgwLRSvJDXVe+DYrp8c3vbdTO8+x0Uclhnee4KHg5SWEFqkclZ8whVVE68Il08NBKu1QMxPPPo4R+H8CKDNv/Hs1Irm36TZzxcTA/47KtiWNI1ePdsFUnUf3dxz/Hk91cLvUB2JDCNwzAEsmSWlqQUrApHGQBgcKxeK/1CMj3MU0jadqlw3Z+dwzCttEnlPrEC8k2CZbeVMvc/ItuvpNtvKggIOu0ZuYTQcy0Pa0vngcn4ixMe5C49FNPLbpUf6ez9wk+CtIhR1NNQq4GTMkXAuMNJRPnz4KIUfV/nwMbb4/80+apsVZWgOAAAAAElFTkSuQmCC)</a>"
      ],
      "metadata": {
        "id": "-hJm9VY0E_9_"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (main, Dec 15 2022, 18:18:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}